#!/usr/bin/python

import argparse
import sys
import urllib
import re
try:
    from bs4 import BeautifulSoup
    import requests
except:
    print "You're missing libraries"


def spider_url(aurl, afile):
    """Spider our given URL and dump links to file"""
    print "Spidering: " + aurl
    headers = {"User-agent": "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"}
    r = requests.get(aurl, headers=headers)
    links = []
    if r.status_code == 200:
        html = r.text
        bs = BeautifulSoup(html)
        for link in bs.find_all('a'):
            links.append((link.get('href')))
    else:
        print "We got something other than a 200"

    count = 0
    with open(afile, 'w') as fh:
        for link in links:
            count += 1
            if link is None:
                pass
            else:
                if re.match("^http", link):
                    fh.write(link)
                    fh.write("\n")
                elif re.match("^/", link):
                    full_url = aurl + link
                    fh.write(full_url)
                    fh.write("\n")
    fh.close()

    print "\n [ Analysis complete...file written. ]"
    print "\n [ Total links: " + str(count) + "]"


def __main__():
    """Get this party started"""
    parser = argparse.ArgumentParser(description='Basic site spiderererer')
    parser.add_argument('--url', '-u', dest='url', help='Base URL to start spidering')
    parser.add_argument('--file', '-f', dest='file', default='urls.txt', help='File to dump output to')
    parser.add_argument('--version', '-v', action='version', version='%(prog)s 0.1')
    args = parser.parse_args()
    afile = args.file
    aurl = args.url

    if not args.url:
        sys.exit(parser.print_help())
    else:
        spider_url(aurl, afile)


if __name__ == '__main__':
    __main__()

