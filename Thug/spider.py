#!/usr/bin/python

import argparse
import sys
import urllib
try:
    from bs4 import BeautifulSoup
    import requests
except:
    print "You're missing libraries"


def spider_url(aurl, afile):
    """Spider our given URL and dump links to file"""
    print "Spidering: " + aurl
    headers = {"User-agent": "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"}
    r = requests.get(aurl, headers=headers)
    links = []
    if r.status_code == 200:
        html = r.text
        bs = BeautifulSoup(html)
        for link in bs.find_all('a'):
            links.append((link.get('href')))
    else:
        print "We got something other than a 200"

    count = 0
    for link in links:
        count += 1
        print link

    print "Total links: " + str(count)


def __main__():
    """Get this party started"""
    parser = argparse.ArgumentParser(description='Basic site spiderererer')
    parser.add_argument('--url', '-u', dest='url', help='Base URL to start spidering')
    parser.add_argument('--file', '-f', dest='file', default='urls.txt', help='File to dump output to')
    parser.add_argument('--version', '-v', action='version', version='%(prog)s 0.1')
    args = parser.parse_args()
    afile = args.file
    aurl = args.url

    if not args.url:
        sys.exit(parser.print_help())
    else:
        spider_url(aurl, afile)


if __name__ == '__main__':
    __main__()

