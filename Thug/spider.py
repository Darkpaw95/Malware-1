#!/usr/bin/python
# Spider script to dump all URLs to file

import argparse
import sys
import urllib
import re
from urlparse import urlparse
try:
    from bs4 import BeautifulSoup
    import requests
except:
    print "You're missing libraries"


def check_blacklist(aurl, ablacklist):
    """Check if a given URL is on our blacklist"""
    checkurl = urlparse(aurl)
    goodurl = checkurl.hostname
    blacklist = []
    try:
        with open(ablacklist, 'r') as fh:
            for line in fh:
                parse_url = urlparse(line)
                blacklist.append(parse_url.hostname)
    except:
        sys.exit("Blacklist file doesn't exist")

    for url in blacklist:
        if goodurl == url:
            blacklisted = 1
        else:
            blacklisted = 0

    return blacklisted


def spider_url(aurl, afile, ablacklist):
    """Spider our given URL and dump links to file"""
    print "Spidering: " + aurl + "\n"
    headers = {"User-agent": "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"}
    r = requests.get(aurl, headers=headers)
    links = []
    if r.status_code == 200:
        html = r.text
        bs = BeautifulSoup(html)
        for link in bs.find_all('a'):
            links.append((link.get('href')))
    else:
        print "We got something other than a 200"

    # Parse our HTML file and dump links to an list
    count = 0
    urls = []
    for link in links:
        count += 1
        if link is None:
            pass
        else:
            if re.match("^http", link):
                if link not in urls:
                    urls.append(link)
            elif re.match("^/", link):
                full_url = aurl + link
                if full_url not in urls:
                    urls.append(full_url)

    # Do a little house cleaning
    added = 0
    print " * Found URLS...pushing to file for analysis\n"
    with open(afile, 'w') as fh:
        for url in urls:
            blchk = check_blacklist(url, ablacklist)
            if blchk == 0:
                print url
                added += 1
                print url + " ...adding"
                fh.write(url)
                fh.write("\n")
            else:
                print url + " ...blacklisted"
    fh.close()

    print "\n [ Analysis complete...file written. ]"
    print " [ Total links: " + str(count) + "]"
    print " [ Added for analysis: " + str(added) + "]"


def __main__():
    """Get this party started"""
    parser = argparse.ArgumentParser(description='Basic site spiderererer')
    parser.add_argument('--url', '-u', dest='url', help='Base URL to start spidering')
    parser.add_argument('--file', '-f', dest='file', default='urls.txt', help='File to dump output to')
    parser.add_argument('--blacklist', '-b', dest='blacklist', default='blacklist.txt', help='File to pull our blacklisted URLs from')
    parser.add_argument('--version', '-v', action='version', version='%(prog)s 0.1')
    args = parser.parse_args()
    afile = args.file
    aurl = args.url
    global ablacklist
    ablacklist = args.blacklist

    if not args.url:
        sys.exit(parser.print_help())
    else:
        spider_url(aurl, afile, ablacklist)


if __name__ == '__main__':
    __main__()
