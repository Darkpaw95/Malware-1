#!/usr/bin.python
# Simple script to pull down a remote URL for analysis with Thug

import argparse
import sys
import os
import hashlib
import urllib
try:
    import requests
except:
    sys.exit("You need the requests library")


def dup_check():
    """Check if we've already pulled down a copy"""
    print "Checking for dups"


def hash_file(urldump):
    """Return the hash value for our dumped file"""
    print "Calculating hash for file" 

    return md5hash


def url_pull(url, adest):
    """Pull down our URLs for review"""
    print "Pulling: " + url 
    md5url = hashlib.md5(url).hexdigest()
    print "Saving to file: " + md5url
    #encoded_url = urllib.quote_plus(url) 
    headers = { "User-agent" : "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)" }
    r = requests.get(url, headers=headers)
    fullpath = adest + "/" + md5url
    with open(fullpath, 'w') as urldump:
        urldump.write(r.text)
        urldump.close()
    #hash_value = hash_file(urldump)
    #print str(hash_value)


def load_file(afile, adest):
    """Load our URLs from file"""
    print "Dump directory: " + adest
    with open(afile, 'r') as fh:
        for url in fh:
            clean_url = url.rstrip()
            url_pull(clean_url, adest)
    print "Dumped URLs to file"


def __main__():
    """Get this party started"""
    parser = argparse.ArgumentParser(description='URL Puller')
    parser.add_argument('--file', '-f', dest='file', help='File with URLs to pull down')
    parser.add_argument('--dest', '-d', dest='dest', default='/tmp/', help='Directory directory to dump to')
    parser.add_argument('--version', '-v', action='version', version='%(prog)s 0.1')
    args = parser.parse_args()
    afile = args.file
    global adest
    adest = args.dest

    if not args.file:
        sys.exit(parser.print_help())
    else:
        load_file(afile, adest)

if __name__ == '__main__':
    __main__()

