#!/usr/bin.python
# Simple script to pull down a remote URL for analysis with Thug

import argparse
import sys
import os
import hashlib
try:
    import requests
except:
    sys.exit("You need the requests library")


def dup_check():
    """Check if we've already pulled down a copy"""
    print "Checking for dups"


def url_pull(url, adest):
    """Pull down our URLs for review"""
    print "Pulling: " + url,
    md5url = hashlib.md5(url).hexdigest()
    print " (" + md5url + ")"
    headers = {"User-agent": "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"}
    r = requests.get(url, headers=headers)
    fullpath = adest + "/" + md5url
    oldfile = os.path.exists(fullpath)
    if oldfile:
	print "Existing URL Hash Found: ",
	# Hash our old file
	file = open(fullpath, 'rb')
	m = hashlib.md5()
	m.update(file.read())
	odigest = m.hexdigest()
	file.close()

	# Pull down a new version
	newfile = adest + "/" + md5url + "-new"
	with open(newfile, 'w') as new:
	    new.write(r.content)
	    new.close()

	# Hash our new file
	newhf = adest + "/" + md5url + "-new"
	file = open(newhf, 'rb')
	m = hashlib.md5()
	m.update(file.read())
	ndigest = m.hexdigest()
	file.close()

	if ndigest == odigest:
	    print "Hashes match. Site content hasn't changed.\n"
	    os.remove(newhf)
	else:
	    print "Hashes don't match. Rescanning site content\n"
	    os.remove(fullpath)
	    os.rename(newhf, fullpath)

    else:
	print "No existing site reviews"
	with open(fullpath, 'w') as urldump:
	    urldump.write(r.content)
	    urldump.close()
	print "Hashing",
	with open(fullpath) as newfile:
	    file = open(fullpath)
	    m = hashlib.md5()
	    m.update(file.read())
	    digest = m.hexdigest()
	    file.close()
	    newhash = digest
	    print "Hash: " + newhash
	print "...ok"


def load_file(afile, adest):
    """Load our URLs from file"""
    with open(afile, 'r') as fh:
        for url in fh:
            clean_url = url.rstrip()
            url_pull(clean_url, adest)
    print "\nDumped URLs to file for Thug Analysis"
    print "Output directory: " + adest


def __main__():
    """Get this party started"""
    parser = argparse.ArgumentParser(description='URL Puller')
    parser.add_argument('--file', '-f', dest='file', help='File with URLs to pull down')
    parser.add_argument('--dest', '-d', dest='dest', default='/tmp/', help='Directory directory to dump to')
    parser.add_argument('--version', '-v', action='version', version='%(prog)s 0.1')
    args = parser.parse_args()
    afile = args.file
    global adest
    adest = args.dest

    if not args.file:
        sys.exit(parser.print_help())
    else:
	print "[ Thug URL dumper ]\n"
        load_file(afile, adest)

if __name__ == '__main__':
    __main__()